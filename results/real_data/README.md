# Real-data SeriesHMM TinyMoA vs TinyRNN Results

This report mirrors the synthetic benchmarking summary for the Mixture-of-Agents style
behavioural sessions. The committed run was executed in demo mode inside this repository,
so the metrics reflect a surrogate long-dwell two-step generator; swapping in a converted
`.npz` bundle from the real dataset will populate the same structure with genuine results.

## Run configuration

- Command: `python -m series_hmm_rnn.run_real_data_pipeline --demo-synthetic --epochs 5 --out-dir results/real_data/demo --device cpu`
- Training epochs: 5 with Adam (lr = 1e-3)
- Hidden units: SeriesHMM-TinyMoA = 6, SeriesHMM-TinyRNN = 6, phases `K = 2`, temperature `tau = 1.25`
- Hold-out split: 80% sessions train / 20% test (rounded to integers)
- Sticky initial transition prior: 0.97 self-transition probability

## Evaluation metrics

| Model              | Split | NLL  | Action accuracy | Phase accuracy |
|--------------------|-------|------|-----------------|----------------|
| SeriesHMM-TinyMoA  | Train | 0.920 | 0.567           | 0.538          |
| SeriesHMM-TinyMoA  | Test  | 0.909 | 0.575           | 0.635          |
| SeriesHMM-TinyRNN  | Train | 0.656 | 0.528           | 0.935          |
| SeriesHMM-TinyRNN  | Test  | 0.667 | 0.490           | 0.935          |

## Visual summaries

- [Training NLL](./demo_fig/real_demo_train_nll.svg) / [Training accuracy](./demo_fig/real_demo_train_accuracy.svg)
- [Evaluation accuracy (train/test)](./demo_fig/real_demo_action_accuracy.svg) / [Phase alignment](./demo_fig/real_demo_phase_accuracy.svg)
  overlays observed behaviour, both SeriesHMM models, and the four constituent agents.

The trial-history curves fit a simple logistic regression on sequences sampled from each
model (arg-max policy) alongside the Model-free Reward, Model-free Choice, Model-based,
and Bias agents. Observed data use the ground-truth actions; all model/agent traces rely on
the recorded rewards as proxies for outcomes, matching the conventions in the SeriesHMM
Mixture-of-Agents literature.

## Artefacts committed for reference

- [`demo/`](./demo/) stores `history.json`, `metrics.json`, `posterior_trace.json`, and `trial_history.json` for both models and agents.
- [`demo_fig/`](./demo_fig/) contains the SVG plots generated by
  `python scripts/plot_synthetic_results.py results/real_data/demo --out-dir results/real_data/demo_fig --prefix real_demo`.
- [`comparison.md`](./comparison.md) narrates the new MoA versus TinyRNN responsibility comparison figures.
- [`sample_trace.json`](./demo/sample_trace.json) captures a representative session (actions,
  rewards, transitions, phases) used when constructing the visualisations and logistic fits.

## Reproducing with your own dataset

1. Convert the MixtureAgentsModels MATLAB exports into a NumPy bundle as described in
   [`scripts/convert_mixture_agents.py`](../../scripts/convert_mixture_agents.py).
2. Run the pipeline, pointing `--data` to the converted `.npz` file and adjusting `--epochs`
   as needed. All logs and figures will appear under your chosen `--out-dir`.
3. Regenerate the figures (including the trial-history regressions) with
   `python scripts/plot_synthetic_results.py <out-dir> --out-dir fig --prefix <name>`.

Replace the demo artefacts in this folder with the outputs from the real dataset to keep
synthetic and empirical comparisons aligned.
