# HMM-MoA vs HMM-TinyRNN responsibility comparison

This note condenses the figures under `results/real_data/demo_fig` that expose how
the hybrid models behave internally on the demo real-data run. Each section embeds
the generated SVG and highlights the takeaways so the plots can be dropped straight
into a presentation.

## HMM-MoA agent responsibility (`real_demo_agent_mix_hmm_moa.svg`)

![HMM-MoA agent mix](./demo_fig/real_demo_agent_mix_hmm_moa.svg)

*Reading the plot*: the four trajectories correspond to the default Mixture-of-Agents
experts (MF reward, MF choice, model-based, bias). Values are normalised to sum to
one at every trial. The shaded band beneath the axes is colour-coded by the most
probable expert so you can immediately spot when responsibility shifts, while the
overlayed strip shows the ground-truth phase from the demo generator.

*What it shows*: in this demo session the model-based and bias experts trade off
control. The bias head dominates 107 trials while the model-based head wins 93,
leaving only small windows where the model-free components take charge. Averaged
across the run the weights settle near 0.16 (MF reward), 0.22 (MF choice), 0.30
(model-based), and 0.31 (bias), underscoring that the hybrid leans heavily on the
planning-style and static-bias policies.【F:results/real_data/demo/hmm_moa/posterior_trace.json†L1-L200】 Because the surrogate
dataset is generated by a model-based decision rule, the "ground truth" agent
would hug the model-based corner (weight 1.0 for that policy across all trials). The
ground-truth phase overlay therefore makes it obvious when the learned gating leans
on the wrong expert.【F:series_hmm_rnn/data.py†L1-L25】

## HMM-TinyRNN projected agent responsibility (`real_demo_agent_mix_hmm_tinyrnn.svg`)

![HMM-TinyRNN projected mix](./demo_fig/real_demo_agent_mix_hmm_tinyrnn.svg)

*Reading the plot*: to make the TinyRNN directly comparable to the MoA head, the
plot projects its per-trial action logits onto the same four baseline agents.
For each trial we solve a small ridge-regularised system that enforces
non-negative, sum-to-one weights while matching the TinyRNN policy gradient with
the agent Q-value differences stored in `baseline_q`. The coloured curves are the
resulting mixture weights, and the dominance band again highlights the most
responsible agent per trial with the ground-truth phase overlay above it.【F:results/real_data/demo/hmm_tinyrnn/posterior_trace.json†L1-L200】【F:results/real_data/demo/hmm_tinyrnn/posterior_trace.json†L201-L360】

*What it shows*: even without explicit gating, the TinyRNN leans heavily on the
model-based and bias-like directions implied by the agent library. Averaged over
the session the projection assigns weights of roughly 0.31 (MF reward), 0.00 (MF
choice), 0.32 (model-based), and 0.36 (bias), and the dominance ribbon spends 103
trials in the bias corner versus 65 for MF reward and 32 for the model-based
policy.【F:results/real_data/demo/hmm_tinyrnn/posterior_trace.json†L1-L360】 The vanishing MF-choice
component matches the intuition that the RNN absorbs most of its flexibility from
value-sensitive signals rather than perseveration.

## HMM-MoA latent-state posterior (`real_demo_state_posterior_hmm_moa.svg`)

![HMM-MoA state posterior](./demo_fig/real_demo_state_posterior_hmm_moa.svg)

The HMM backbone spends roughly 81% of the session in a single phase, with only
brief excursions into the alternate state before snapping back.【F:results/real_data/demo/hmm_moa/posterior_trace.json†L1-L200】
This makes it easy to spot when the MoA gating shifts responsibility away from its
default regime. The translucent band above the dominant-state stripe encodes the
ground-truth phase annotations recorded in the dataset so you can see, at a glance,
how often the inferred posterior agrees with reality.【F:results/real_data/demo/hmm_moa/posterior_trace.json†L801-L1040】

## HMM-TinyRNN latent-state posterior (`real_demo_state_posterior_hmm_tinyrnn.svg`)

![HMM-TinyRNN state posterior](./demo_fig/real_demo_state_posterior_hmm_tinyrnn.svg)

The RNN variant discovers the same two-phase structure but allocates time
differently: the second state carries ~71% of the posterior mass, leaving the
first state to handle short bursts early in the session.【F:results/real_data/demo/hmm_tinyrnn/posterior_trace.json†L1-L200】 The
same ground-truth overlay appears here; mismatched colours between the top band and
the translucent strip highlight trials where the models disagree with the known
phase identity.【F:results/real_data/demo/hmm_tinyrnn/posterior_trace.json†L801-L1040】 Contrasting the MoA and TinyRNN posteriors
makes it easy to flag trials where the two models disagree about which regime
generated the observed behaviour.

To regenerate these figures after rerunning the pipeline, execute:

```bash
python scripts/plot_synthetic_results.py results/real_data/demo \
  --out-dir results/real_data/demo_fig --prefix real_demo
```

and refresh this document if you swap in a new dataset.
